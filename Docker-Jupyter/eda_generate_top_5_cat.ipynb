{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0387b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_parquet('data/processed/final_joined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "\n",
    "# Check original size\n",
    "before_rows = len(df)\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Report result\n",
    "after_rows = len(df)\n",
    "removed = before_rows - after_rows\n",
    "\n",
    "print(f\"Removed {removed:,} duplicate rows. Final row count: {after_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'title_review', 'text', 'images_review', 'description',\n",
    "    'bought_together', 'subtitle', 'author', 'main_category',\n",
    "    'asin', 'parent_asin', 'user_id', 'title_meta', 'features',\n",
    "    'images_meta', 'store', 'categories'\n",
    "]\n",
    "\n",
    "# Track dropped or missing columns\n",
    "dropped_cols = []\n",
    "\n",
    "# Define helper to detect empty lists or their string form\n",
    "def is_effectively_empty_list(val):\n",
    "    return val == [] or val == '[]' or str(val).strip().lower() in {'[]', 'none', ''}\n",
    "\n",
    "for col in columns_to_check:\n",
    "    if col in df.columns:\n",
    "        # Replace only effectively empty list-like values with NaN\n",
    "        df[col] = df[col].apply(lambda x: np.nan if is_effectively_empty_list(x) else x)\n",
    "\n",
    "        # Count nulls and calculate usable %\n",
    "        null_count = df[col].isna().sum()\n",
    "        total_rows = len(df)\n",
    "        usable_pct = 100 * (1 - null_count / total_rows)\n",
    "\n",
    "        print(f\"{col} → Nulls: {null_count:,} / {total_rows:,} → Usable: {usable_pct:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' no longer exists in the DataFrame.\")\n",
    "        dropped_cols.append(col)\n",
    "\n",
    "# Summary of missing columns\n",
    "if dropped_cols:\n",
    "    print(f\"\\nSummary: The following columns were not found (reason: dropped – verify in the next cell): {', '.join(dropped_cols)}\")\n",
    "else:\n",
    "    print(\"\\nAll specified columns are present and processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f46b7",
   "metadata": {},
   "source": [
    "### Based on above result - Confirm columns to drop \n",
    "\n",
    "1. images_review\n",
    "2. bought_together\n",
    "3. subtitle\n",
    "4. author\n",
    "\n",
    "In addition: <br>\n",
    "5. details <br>\n",
    "6. average_rating #not applicable since its 1.5M of entire dataset only <br>\n",
    "7. rating_number #not applicable since its 1.5M of entire dataset only <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to drop\n",
    "processed_drop = ['images_review', 'bought_together', 'subtitle', 'author', 'details', 'average_rating', 'rating_number', 'videos']\n",
    "\n",
    "# Drop only if they exist\n",
    "existing_cols = [col for col in processed_drop if col in df.columns]\n",
    "df.drop(columns=existing_cols, inplace=True)\n",
    "\n",
    "# Confirm drop\n",
    "print(f\"Dropped columns: {existing_cols}\")\n",
    "print(f\"Remaining columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns setup\n",
    "id_columns = {'asin', 'parent_asin', 'user_id'}\n",
    "preserve_bracket_fields = {'description', 'features', 'images_meta', 'categories'}\n",
    "text_lower_fields = {'title_review', 'text', 'title_meta', 'store'}\n",
    "\n",
    "# Values to treat as null\n",
    "null_like_values = {'none', '', ' ', '[]'}\n",
    "\n",
    "# Cleaning function\n",
    "def smart_clean(val, col):\n",
    "    # ID columns: trim + set NaN if empty-like\n",
    "    if col in id_columns:\n",
    "        if isinstance(val, str):\n",
    "            trimmed = val.strip()\n",
    "            return np.nan if trimmed.lower() in null_like_values else trimmed\n",
    "        return val\n",
    "\n",
    "    # Bracket-sensitive: lowercase + NaN if empty-like\n",
    "    if col in preserve_bracket_fields:\n",
    "        if isinstance(val, str):\n",
    "            raw = val.strip()\n",
    "            return np.nan if raw.lower() in null_like_values else raw.lower()\n",
    "        elif isinstance(val, list):\n",
    "            return [str(item).strip().lower() for item in val if isinstance(item, str)]\n",
    "        return val\n",
    "\n",
    "    # Regular text fields\n",
    "    if isinstance(val, str):\n",
    "        val = val.strip().lower()\n",
    "        return np.nan if val in null_like_values else val\n",
    "\n",
    "    return val\n",
    "\n",
    "# Apply per column\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: smart_clean(x, col))\n",
    "\n",
    "print(\"Cleaned DataFrame: bracket fields preserved, lowercase applied, nulls handled.\")\n",
    "\n",
    "# Ensure numeric first, then convert to int\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec34f2",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a52746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine 'features' & 'description'\n",
    "sample_feature = df['features'].dropna().iloc[0]\n",
    "sample_description = df['description'].dropna().iloc[0]\n",
    "\n",
    "print(sample_feature)\n",
    "print(sample_description)\n",
    "\n",
    "# Perform clean up for 'features' & 'description'\n",
    "def clean_stringified_list(val):\n",
    "    \"\"\"\n",
    "    Cleans stringified list fields like 'features' and 'description'.\n",
    "    Returns a flattened, SBERT-friendly string.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        if isinstance(parsed, list):\n",
    "            return \" \".join([str(s).strip() for s in parsed])\n",
    "    except:\n",
    "        pass\n",
    "    return str(val).strip()\n",
    "\n",
    "# Apply separately to create two new clean columns\n",
    "df['features_clean'] = df['features'].apply(clean_stringified_list)\n",
    "df['description_clean'] = df['description'].apply(clean_stringified_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine 'images_meta'\n",
    "sample_meta = df['images_meta'].dropna().iloc[0]\n",
    "print(sample_meta)\n",
    "\n",
    "'''\n",
    "Transformed images_meta into 3 structured fields for downstream use:\n",
    "- main_image_url (best image based on priority: hi_res > large > thumb)\n",
    "- num_images (total image count)\n",
    "- hi_res_images (list of hi-res or fallback large URLs)\n",
    "'''\n",
    "\n",
    "def extract_main_image_url(val):\n",
    "    try:\n",
    "        images = ast.literal_eval(val)\n",
    "        if isinstance(images, list):\n",
    "            for img in images:\n",
    "                if img.get('variant') == 'main':\n",
    "                    return img.get('hi_res') or img.get('large') or img.get('thumb')\n",
    "            # fallback to first image\n",
    "            img = images[0]\n",
    "            return img.get('hi_res') or img.get('large') or img.get('thumb')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Count total images\n",
    "def count_images(val):\n",
    "    try:\n",
    "        images = ast.literal_eval(val)\n",
    "        return len(images) if isinstance(images, list) else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Extract all hi_res (or fallback to large)\n",
    "def extract_all_hi_res(val):\n",
    "    try:\n",
    "        images = ast.literal_eval(val)\n",
    "        return [img.get('hi_res') or img.get('large') for img in images if isinstance(img, dict)]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply all 3\n",
    "df['main_image_url'] = df['images_meta'].apply(extract_main_image_url)\n",
    "df['num_images'] = df['images_meta'].apply(count_images)\n",
    "df['hi_res_images'] = df['images_meta'].apply(extract_all_hi_res)\n",
    "\n",
    "print(\"Extracted image metadata fields: main_image_url, num_images, hi_res_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8433dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp_utc'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d245dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b432f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_categories(x):\n",
    "    try:\n",
    "        return json.loads(x) if isinstance(x, str) and x.strip().startswith('[') else []\n",
    "    except:\n",
    "        return []\n",
    "df['parsed_categories'] = df['categories'].apply(safe_parse_categories)\n",
    "\n",
    "df['category_depth'] = df['parsed_categories'].apply(len)\n",
    "\n",
    "# Value counts of category depth\n",
    "print(\"Category Depth Distribution:\")\n",
    "print(df['category_depth'].value_counts().sort_index())\n",
    "\n",
    "df['category_depth'].value_counts().sort_index().plot(kind='bar', title='Category Depth Distribution')\n",
    "plt.xlabel(\"Number of Levels\")\n",
    "plt.ylabel(\"Number of Products\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc3a4b",
   "metadata": {},
   "source": [
    "### From above plot, to extract up to 5 levels (cat_0 to cat_4 for column 'categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fef5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verified_purchase_flag'] = df['verified_purchase'].apply(lambda x: 1 if x is True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_category_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    elif isinstance(val, str) and val.strip().startswith(\"[\"):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            return parsed if isinstance(parsed, list) else []\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# Parse and extract category levels\n",
    "df['parsed_categories'] = df['categories'].apply(parse_category_list)\n",
    "\n",
    "for i in range(5):\n",
    "    df[f'cat_{i}'] = df['parsed_categories'].apply(\n",
    "        lambda lst: lst[i].strip().lower() if i < len(lst) else np.nan\n",
    "    )\n",
    "\n",
    "# Drop parsed_categories to clean up\n",
    "df.drop(columns=['parsed_categories'], inplace=True)\n",
    "\n",
    "print(\"Extracted cat_0 to cat_4 and dropped 'parsed_categories'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7314de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = ['verified_purchase', 'main_category', 'categories', 'asin', 'features', 'description', 'images_meta', 'timestamp']\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "# Log transform price column\n",
    "df['price_log'] = np.log1p(df['price'])\n",
    "\n",
    "# Create new feature BPR implementation (Rationale: Plot below)\n",
    "df['helpful_vote_clipped'] = df['helpful_vote'].clip(upper=5)\n",
    "\n",
    "# Feature engineer a new average_rating column by parent\n",
    "df['avg_rating_parent'] = df.groupby('parent_asin')['rating'].transform('mean').round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea82269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where cat_0 is nan\n",
    "df = df[~df['cat_0'].isna()].reset_index(drop=True)\n",
    "\n",
    "# Perform imputation for cat_4 (fallback to cat_3 if NaN)\n",
    "\n",
    "# Fill NaN values in cat_4 with values from cat_3\n",
    "df['cat_4'] = df['cat_4'].fillna(df['cat_3'])\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "after = len(df)\n",
    "removed = before - after\n",
    "\n",
    "print(f\"Removed {removed:,} duplicate rows. Final row count: {after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['cat_0', 'cat_1', 'cat_2', 'cat_3', 'cat_4']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nTop categories in {col}:\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809462d",
   "metadata": {},
   "source": [
    "## From above, confirmed that the above cat_0 can be considered as 'sports & outdoors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore its safe to drop cat_0 now\n",
    "\n",
    "df.drop(columns=['cat_0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4986a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8eec1d",
   "metadata": {},
   "source": [
    "## Analysis of Final Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types in DataFrame:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDistribution of helpful_vote:\")\n",
    "print(df['helpful_vote'].describe())\n",
    "\n",
    "print(\"\\nDistribution of rating:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nPrice Statistics:\")\n",
    "print(df['price'].describe())\n",
    "\n",
    "min_price = df['price'].min()\n",
    "max_price = df['price'].max()\n",
    "print(f\"\\nPrice Range: ${min_price:.2f} to ${max_price:.2f}\")\n",
    "\n",
    "print(\"\\nLog-Transformed Price Statistics:\")\n",
    "print(df['price_log'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['cat_1', 'cat_2', 'cat_3', 'cat_4']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nTop categories in {col}:\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf619e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given there are NaN in cat_3 and cat_4, will perform backfill referencing from cat_2\n",
    "\n",
    "df['cat_3'] = df['cat_3'].fillna(df['cat_2'])\n",
    "df['cat_4'] = df['cat_4'].fillna(df['cat_3'])  # This now fills from the new cat_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['cat_1', 'cat_2', 'cat_3', 'cat_4']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nTop categories in {col}:\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 row with 4 subplots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Top categories in cat_1\n",
    "df['cat_1'].value_counts(dropna=False).head(10).plot(\n",
    "    kind='barh', ax=axes[0], color='darkorange'\n",
    ")\n",
    "axes[0].set_title(\"Top Categories - cat_1\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Top categories in cat_2\n",
    "df['cat_2'].value_counts(dropna=False).head(10).plot(\n",
    "    kind='barh', ax=axes[1], color='seagreen'\n",
    ")\n",
    "axes[1].set_title(\"Top Categories - cat_2\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Top categories in cat_3\n",
    "df['cat_3'].value_counts(dropna=False).head(10).plot(\n",
    "    kind='barh', ax=axes[2], color='skyblue'\n",
    ")\n",
    "axes[2].set_title(\"Top Categories - cat_3\")\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "# Top categories in cat_4\n",
    "df['cat_4'].value_counts(dropna=False).head(10).plot(\n",
    "    kind='barh', ax=axes[3], color='orchid'\n",
    ")\n",
    "axes[3].set_title(\"Top Categories - cat_4\")\n",
    "axes[3].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e530432",
   "metadata": {},
   "source": [
    "### Based on the above plot, cold start basket collection will be:\n",
    "1. camping & hiking\n",
    "2. cycling\n",
    "3. fitness technology\n",
    "4. strength training equipment\n",
    "5. cardio training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4754f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top10_avg_rating_by_category(cat_col, color, ax):\n",
    "    top_10 = df[cat_col].value_counts().head(10).index\n",
    "    avg_rating = (\n",
    "        df[df[cat_col].isin(top_10)]\n",
    "        .groupby(cat_col)['rating']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    avg_rating.plot(kind='bar', color=color, ax=ax)\n",
    "    ax.set_title(f\"Avg Rating by Top 10 {cat_col} Categories\")\n",
    "    ax.set_ylabel(\"Average Rating\")\n",
    "    ax.set_xlabel(cat_col)\n",
    "    ax.set_xticklabels(avg_rating.index, rotation=45, ha='right')\n",
    "\n",
    "    for i, v in enumerate(avg_rating.values):\n",
    "        ax.text(i, v + 0.03, f\"{v:.2f}\", ha='center', fontsize=8)\n",
    "\n",
    "# Create subplots in a single horizontal row\n",
    "fig, axes = plt.subplots(1, 4, figsize=(28, 10))  # Wider width to fit 4 plots\n",
    "\n",
    "# Plot each category level\n",
    "plot_top10_avg_rating_by_category('cat_1', 'salmon', axes[0])\n",
    "plot_top10_avg_rating_by_category('cat_2', 'mediumseagreen', axes[1])\n",
    "plot_top10_avg_rating_by_category('cat_3', 'skyblue', axes[2])\n",
    "plot_top10_avg_rating_by_category('cat_4', 'orchid', axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aac2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Clipped at 5\n",
    "df['helpful_vote'].clip(upper=5).hist(bins=6, ax=axes[0])\n",
    "axes[0].set_title(\"Helpful Votes (Capped at 5)\")\n",
    "axes[0].set_xlabel(\"helpful_vote\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: Unclipped\n",
    "df['helpful_vote'].hist(bins=100, ax=axes[1])\n",
    "axes[1].set_title(\"Helpful Votes (Full Range)\")\n",
    "axes[1].set_xlabel(\"helpful_vote\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Rating distribution\n",
    "df['rating'].value_counts().sort_index().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title(\"Rating Distribution\")\n",
    "axes[0].set_xlabel(\"Rating\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: Price (clipped at $200)\n",
    "df['price'].clip(upper=200).hist(bins=50, ax=axes[1])\n",
    "axes[1].set_title(\"Price Distribution (Capped at $200)\")\n",
    "axes[1].set_xlabel(\"Price ($)\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot 3: Price (full range)\n",
    "df['price'].hist(bins=100, ax=axes[2])\n",
    "axes[2].set_title(\"Price Distribution (Full Range)\")\n",
    "axes[2].set_xlabel(\"Price ($)\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24dcac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c13cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to be filtered with only rows containing the top 5 categories\n",
    "top_cat2 = [\n",
    "    \"camping & hiking\",\n",
    "    \"cycling\",\n",
    "    \"fitness technology\",\n",
    "    \"strength training equipment\",\n",
    "    \"cardio training\"\n",
    "]\n",
    "\n",
    "# Filter the DataFrame\n",
    "top_5_df = df[df['cat_2'].isin(top_cat2)].copy()\n",
    "\n",
    "# Output the result\n",
    "print(f\"Filtered top_5_df created with {top_5_df.shape[0]:,} rows and {top_5_df.shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508920b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of rows\n",
    "total_rows = len(top_5_df)\n",
    "print(f\"Total rows in top_5_df: {total_rows:,}\")\n",
    "\n",
    "# Total number of rows\n",
    "total_rows = len(df)\n",
    "print(f\"Total rows in df: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b614ff",
   "metadata": {},
   "source": [
    "# Export 2 files\n",
    "- Final_Table (labelled as: df) - total of 1,487,892 (backup in case phase 2 team needs more than 5 categories)\n",
    "- 5_Category_Final_Table (labelled as: top_5_df) - 767,684"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd64242",
   "metadata": {},
   "source": [
    "### Please note the nan values existing across these columns - to decide if drop rows, imputation etc. to proceed with model development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c77a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute missing values for df\n",
    "df_na = df.isna().sum().reset_index()\n",
    "df_na.columns = ['Column', 'NaN in df']\n",
    "df_na = df_na[df_na['NaN in df'] > 0]  # optional: show only columns with NaNs\n",
    "\n",
    "# Compute missing values for top_5_df\n",
    "top_5_na = top_5_df.isna().sum().reset_index()\n",
    "top_5_na.columns = ['Column', 'NaN in top_5_df']\n",
    "top_5_na = top_5_na[top_5_na['NaN in top_5_df'] > 0]  # optional: show only columns with NaNs\n",
    "\n",
    "# Display both\n",
    "print(\"Missing Values in df:\")\n",
    "display(df_na)\n",
    "\n",
    "print(\"\\nMissing Values in top_5_df:\")\n",
    "display(top_5_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top 5 category subset dataset\n",
    "top_5_df.to_parquet(f\"{output_path}/5_category_final_table_(767,684 rows).parquet\", index=False)\n",
    "\n",
    "# Export full cleaned dataset - backup for phase 2 team in case you all need more categories\n",
    "df.to_parquet(f\"{output_path}/final_table_(1,487,892 rows).parquet\", index=False)\n",
    "\n",
    "print(\"Export completed: final_table.parquet and 5_category_final_table.parquet saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_1 = pd.read_parquet('data/5_category_final_table_(767,684 rows).parquet')\n",
    "new_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2 = pd.read_parquet('data/final_table_(1,487,892 rows).parquet')\n",
    "new_2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
